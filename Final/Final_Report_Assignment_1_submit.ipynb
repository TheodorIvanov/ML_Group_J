{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Report Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpoxn9s1Dr0T",
        "colab_type": "text"
      },
      "source": [
        "# CS985/6 Assignment 1 Report\n",
        "\n",
        "Members:\n",
        "\n",
        "* Anja Wyzdak - anja.wyzdak.2019@uni.strath.ac.uk 201990375\n",
        "\n",
        "* Michel Scharnitzki - michel.scharnitzki.2019@uni.strath.ac.uk 201987665\n",
        "\n",
        "* Shivling Gawli - shivling.gawli.2019@uni.strath.ac.uk 201992168\n",
        "\n",
        "* Theodor Ivanov - theodor.ivanov.2019@uni.strath.ac.uk 201989531"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgLgLbDyDsEi",
        "colab_type": "text"
      },
      "source": [
        "## I. Regression Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOfYq1F6DsHy",
        "colab_type": "text"
      },
      "source": [
        "### **1. Introduction**\n",
        "\n",
        "Spotify is the second biggest provider of music streaming services in the world in terms of monthly user base shadowed only by Apple Music. With their vast collection of tracks spanning across genres on the edge of the creative frontier, predicting musical taste and providing salient recommendations to the people who use Spotify’s services has never been more challenging.\n",
        "A good music recommender system focuses on recognising the genre and the corresponding mood of the song so that it can make situation-specific suggestions for the next one. As a purely abstract medium, music proves very hard to classify in terms of genre because of its many nuances. These nuances ultimately contribute to its high dimensionality in terms of attributes that can be explored as classifiers. What is more, suggesting the most representative song from a corpus of titles within a genre is what sets the scene of the listening session thus allowing for its gradual exploration. In order to do that the recommender system is interested in quantifying the absolute popularity of a song. Apart from the main features of the system, recently Spotify has introduced the service Tastebreakers which allows for the exploration of the complement to the taste of the listener thus pushing them outside of their comfort zone – another feature relying heavy on classification and quantifying.\n",
        "Based on these concepts, the current work is interested in exploring different machine learning approaches to predicting a song’s genre and popularity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04pGT4RZDsPr",
        "colab_type": "text"
      },
      "source": [
        "#### *1.1 Description of the data set*\n",
        "\n",
        "For the purposes of the analysis the following libraries were used: pandas, numpy, seaborn, statsmodels, and sklearn. The environment sellected for the development and testing of the script was Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhQfphlfD6K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the Data\n",
        "from google.colab import files # insert json token\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!pip install kaggle\n",
        "!kaggle competitions download -c cs98x-spotify-regression\n",
        "!kaggle competitions download -c cs98xspotifyclassification\n",
        "# Libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import sklearn as skl\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import ensemble\n",
        "from sklearn import linear_model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh107ye5DsVf",
        "colab_type": "text"
      },
      "source": [
        "The data for the analysis was taken from the Spotify Songs by Decade list and consists of 4 sets divided into data for regression and classification each subdivided into sets for training and testing. Both training sets include 453 songs with 15 attributes and the test sets consisted of 114 (regression set) and 113 (classification set) with 14 attributes.\n",
        "The attributes provided included:\n",
        "\n",
        "* Id - an arbitrary unique track identifier \n",
        "* title - track title \n",
        "* artist - singer or band \n",
        "* top genre - genre of the track \n",
        "* year - year of release (or re-release) \n",
        "* bpm - beats per minute (tempo) \n",
        "* nrgy - energy: the higher the value the more energetic \n",
        "* dnce - danceability: the higher the value, the easier it is to dance to this song \n",
        "* dB - loudness (dB): the higher the value, the louder the song \n",
        "* live - liveness: the higher the value, the more likely the song is a live recording \n",
        "* val - valence: the higher the value, the more positive mood for the song \n",
        "* dur - duration: the length of the song \n",
        "* acous - acousticness: the higher the value the more acoustic the song is \n",
        "* spch - speechiness: the higher the value the more spoken word the song contains \n",
        "* pop - popularity: the higher the value the more popular the song is (and the target variable for this problem)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHEr-gTuD_Dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is what we train the regression model on\n",
        "training_data_reg = pd.read_csv('/content/CS98XRegressionTrain.csv', low_memory = False)\n",
        "test_data_reg = pd.read_csv('/content/CS98XRegressionTest.csv', low_memory = False)\n",
        "# This is what we train the classification model on\n",
        "training_data_class = pd.read_csv('/content/CS98XClassificationTrain.csv', low_memory = False)\n",
        "test_data_class = pd.read_csv('/content/CS98XClassificationTest.csv', low_memory = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ruejStJDsdW",
        "colab_type": "text"
      },
      "source": [
        "During the exploration of the data it was noted that some of the attributes were unevenly distributed (Fig. 1), with live and speechiness resembling a logarithmic distribution, and year having strong positive skew. These observations were explained by the facts that all songs have a minimum of 0:01s playtime, some songs are instrumental, and that modern technology allows for more music to be made more affordably thus causing a greater creative output which ultimately pushes the number of new songs up as data moves towards the present day.\n",
        "\n",
        "To identify relationships between the attributes for the regression task, a correlation matrix was created and the correlations between popularity and the rest of the predictors were used as a guide when constructing the models. In the course of the analysis only the strongest predictors were used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g8kdXsiEEsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Histograms\n",
        "for_hist = training_data_reg.select_dtypes(exclude=['object'])\n",
        "for_hist.drop(columns='Id', inplace=True)\n",
        "for_hist.hist(column= for_hist.columns, figsize=(20,14))\n",
        "plt.savefig('hist.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9z_-uBsEKNH",
        "colab_type": "text"
      },
      "source": [
        "Figure 1: Distributions of the attributes\n",
        "![Histograms](https://raw.githubusercontent.com/TheodorIvanov/ML_Group_J/master/hist.jpg)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz_gLZh5EOdG",
        "colab_type": "text"
      },
      "source": [
        "### **2. Regression Models & Results**\n",
        "\n",
        "This section introduces the regression models. Regression tasks usually try to predict an outcome (the dependent variable) in this case the popularity of songs, by employing other attributes (the independent variable). After carefully analysing the data set three central independent variables were identified: dB, dur and acous. DB and duration showed positive correlations of 0.31 and 0.32 respectively. Acousticness showed the highest correlation of the set with -0.44 (Fig. 2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFVkfj7AEcVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing NaNs\n",
        "training_data_reg = training_data_reg.dropna()\n",
        "norm_training_data_reg = pd.DataFrame(preprocessing.normalize(training_data_reg.select_dtypes(['number']))) # Normalising\n",
        "# Correlation heatmap\n",
        "training_data_reg_cov = training_data_reg.drop(columns=['Id','year']).corr() # # Removing non-predictor columns\n",
        "plt.figure(figsize =(14,11))\n",
        "plt.title('Predictor Correlations')\n",
        "sns.heatmap(training_data_reg_cov, annot=True, center=0, cmap='BrBG_r')\n",
        "plt.savefig('corr.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDu6cKcMEfjO",
        "colab_type": "text"
      },
      "source": [
        "Figure 2: Correlation matrix between the attributes\n",
        "![Correlation Matrix](https://raw.githubusercontent.com/TheodorIvanov/ML_Group_J/master/corr.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ySLmNQzFQOM",
        "colab_type": "text"
      },
      "source": [
        "#### *2.1 Linear Regression*\n",
        "\n",
        "Linear Regression is one of the most basic regression models. The idea of the linear regression method is to model the relationship between a dependent (endogenous) variable and one or multiple independent (exogenous) variables. This is done by the construction of a regression line that best fits into the data. The best fit is determined by minimising the error between the predicted and observed values. Ultimately, the model tries to minimise the errors (residuals). The residuals are the distance from the regression line to the observed data points. \n",
        "The Kaggle Public RMSE score of the Linear Regression was 8.23155. This number was used as a benchmark for the following models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muJwCHMQEfQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Instalation of the relevant packages\n",
        "#Pandas is a package for spreadsheet analysis\n",
        "#Numpy is a package for computations\n",
        "#Matplotl is a package for data visualization\n",
        "#sklearn is a package for machine learning computation\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# importing the train and the test data\n",
        "#drop the nan values\n",
        "regtraindf = training_data_reg\n",
        "regtraindf.dropna()\n",
        "regtestdf = test_data_reg\n",
        "regtestdf.dropna()\n",
        "# split the sets in train set and test set and remove variables not used for the regression problems\n",
        "X_train = regtraindf.loc[:, \"nrgy\" : \"acous\"]\n",
        "X_train = X_train.drop(['live','val','nrgy','dnce'], axis=1)\n",
        "Y_train = regtraindf.loc[:, \"pop\":\"pop\"]\n",
        "X_test = regtestdf.loc[:, \"nrgy\" : \"acous\"]\n",
        "X_test = X_test.drop(['live','val','nrgy','dnce'], axis=1)\n",
        "\n",
        "X_test = preprocessing.normalize(X_test)\n",
        "X_train = preprocessing.normalize(X_train)\n",
        "\n",
        "# train the model \n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, Y_train)\n",
        "Y_pred = lin_reg.predict(X_test)\n",
        "\n",
        "# create the file in the right format for upload\n",
        "Y_pred = pd.DataFrame(data=Y_pred)\n",
        "Y_pred.columns = ['pop']\n",
        "Id = regtestdf.loc[:, \"Id\" : \"Id\"] \n",
        "result = pd.concat([Id, Y_pred,], axis=1, sort=False)\n",
        "result = result.set_index('Id')\n",
        "result.to_csv('LinReg.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "875d4aByFRbC",
        "colab_type": "text"
      },
      "source": [
        "#### *2.2 Gradient Boosting Methods*\n",
        "\n",
        "Gradient Boosting is one of the most popular hypothesis boosting techniques and it aims at sequentially introducing predictors to an ensemble of weak learners. Unlike its close relative the Adaptive Boosting which focuses on altering the weight of the misclassified instances during training, the Gradient Boosting alters the weight of the residual errors of the previous predictor. It uses a number of decision trees as base learners and can be applied to both classification and regression problems.\n",
        "\n",
        "The performance of the deployed Gradient Boosting Regressor for the popularity prediction showed somewhat less than satisfactory results. Converging on 50 estimators at a learning rate of 0.5, with a total of 3 features and a tree depth of 1, this model scored 51.6% accuracy on the test data and 20.1% on the validation, yielding a Kaggle Public RMSE score of 8.66544."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHx8LiJFFYWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Gradient Boosting: Regression\n",
        "train =  pd.read_csv('/content/CS98XRegressionTrain.csv', low_memory = False)\n",
        "test = pd.read_csv('/content/CS98XRegressionTest.csv', low_memory = False)\n",
        "train.set_index(\"Id\", inplace=True)\n",
        "test.set_index(\"Id\", inplace=True)\n",
        "train.dropna(inplace=True)\n",
        "y_train = train[\"pop\"]\n",
        "\n",
        "# dropping labels\n",
        "train.drop(labels=\"pop\", axis=1, inplace=True)\n",
        "train_test =  train.append(test)\n",
        "\n",
        "# delete columns that are not used as features for training and prediction\n",
        "columns_to_drop = ['title', 'year','top genre','artist']\n",
        "train_test.drop(labels=columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "# Hot one encoding\n",
        "train_test_dummies = train_test\n",
        "\n",
        "# replace nulls with 0.0\n",
        "train_test_dummies.fillna(value=0.0, inplace=True)\n",
        "\n",
        "# generate feature sets (X)\n",
        "X_train = train_test_dummies.values[0:438]\n",
        "X_test = train_test_dummies.values[438:]\n",
        "\n",
        "# scaling the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scale = scaler.fit_transform(X_train)\n",
        "X_test_scale = scaler.transform(X_test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# splitting the data\n",
        "X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train_scale, y_train, random_state=0, shuffle=None)\n",
        "\n",
        "# importing machine learning algorithms\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# computing the accuracy scores on training and validation sets for different learning rates\n",
        "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
        "for learning_rate in learning_rates:\n",
        "    gb = GradientBoostingRegressor(n_estimators=50, learning_rate = learning_rate, max_features=3, max_depth = 1, random_state = 0)\n",
        "    gb.fit(X_train_sub, y_train_sub)\n",
        "    print(\"Learning rate: \", learning_rate)\n",
        "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
        "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
        "    print()\n",
        "\n",
        "# Outputing confusion matrix and classification report of Gradient Boosting algorithm on validation set\n",
        "gb = GradientBoostingRegressor(n_estimators=50, learning_rate = 0.5, max_features=3, max_depth = 1, random_state = 0)\n",
        "gb.fit(X_train_sub, y_train_sub)\n",
        "predictions = gb.predict(X_test_scale)\n",
        "print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
        "print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
        "\n",
        "reg_result = pd.DataFrame()\n",
        "reg_result['pop'] = predictions.T.round(decimals=4)\n",
        "reg_result['Id'] = test.index\n",
        "reg_result.index = reg_result['Id']\n",
        "reg_result.drop(columns='Id',inplace=True)\n",
        "reg_result.to_csv('reg_enc_boost.csv')\n",
        "#!kaggle competitions submit -c cs98x-spotify-regression -f reg_enc_boost.csv -m \"Boost with Encoding\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7es6aBwFf23",
        "colab_type": "text"
      },
      "source": [
        "#### *2.3 Random Forest*\n",
        "\n",
        "The third model used for the regression task was the Random Forest Regression technique. The Random Forest Regression uses multiple decision trees and bootstrap aggregation (bagging). Bagging performs sampling with replacement on different random subsets of the dataset. In the Random Forest Regression each decision tree is trained on a different subset. The multiple decision trees are combined to calculate the final output. In contrast to the Gradient Boosting, Random Forest Regression builds each decision tree independently and combines the results in the end.\n",
        "Grid search was used to find the optimal hyperparameters for the regression model. Grid search cross validates different pre-defined parameters to find the parameters which deliver the highest accuracy levels. Hyperparameter tuning reduced the RMSE score from 8.09130 to 7.9538. In the last step different data scaling techniques were implemented. At this point it is important to notice that this whole process was done in multiple iterations. After each scaling method the grid search function was applied to find the optimal parameters. The optimal parameters for the final Random Forest Regression model were a maximum split of 4 (max_depth = 4) and a number of 50 different trees (n_estimators = 50). The final Kaggle Public RMSE was 7.23191 after employing the normalisation data scaling technique. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6IoVcXSFgO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2.3 Random Forest  Regression\n",
        "#importing the train and the test data\n",
        "#drop the nan values\n",
        "regtraindf = training_data_reg\n",
        "regtraindf.dropna()\n",
        "regtestdf = test_data_reg\n",
        "regtestdf.dropna()\n",
        "#split the sets in train set and test set and remove variables not used for the regression problems\n",
        "\n",
        "X_train = regtraindf.loc[:, \"nrgy\" : \"acous\"]\n",
        "X_train = X_train.drop(['live','val','nrgy','dnce'], axis=1)\n",
        "Y_train = regtraindf.loc[:, \"pop\":\"pop\"]\n",
        "X_test = regtestdf.loc[:, \"nrgy\" : \"acous\"]\n",
        "X_test = X_test.drop(['live','val','nrgy','dnce'], axis=1)\n",
        "\n",
        "\n",
        "#Employ different data scaling techniques\n",
        "#normalize scaler results in RMSE of 7.23191\n",
        "X_test = preprocessing.normalize(X_test)\n",
        "X_train = preprocessing.normalize(X_train)\n",
        "\n",
        "# =============================================================================\n",
        "# #MinMaxScaler results in RMSE of 8.06374\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_train = pd.DataFrame(data=X_train)\n",
        "# X_test = scaler.fit_transform(X_test)\n",
        "# X_test = pd.DataFrame(data=X_test)\n",
        "# \n",
        "# #Normalizer results in RMSE of 7.42832\n",
        "# from sklearn.preprocessing import Normalizer\n",
        "# scaler = Normalizer()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_train = pd.DataFrame(data=X_train)\n",
        "# X_test = scaler.fit_transform(X_test)\n",
        "# X_test = pd.DataFrame(data=X_test)\n",
        "# \n",
        "# #RobustScaler results in RMSE of 7.92187\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "# scaler = RobustScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_train = pd.DataFrame(data=X_train)\n",
        "# X_test = scaler.fit_transform(X_test)\n",
        "# X_test = pd.DataFrame(data=X_test)\n",
        "# =============================================================================\n",
        "\n",
        "#import the RandomForestRegressor package\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "#find the optimal parameters\n",
        "rfc = RandomForestRegressor()\n",
        "#define parameters for optimisation\n",
        "parameters = {\n",
        "    \"n_estimators\":[5,10,50,100,250],\n",
        "    \"max_depth\":[1,2,4,8,16,32,None]\n",
        "\n",
        "}\n",
        "\n",
        "#import the packages for the GridSearchCV optimisaion and import mean_squared_error scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "cv = GridSearchCV(rfc, parameters, scoring=scorer, cv=5)\n",
        "cv.fit(X_train,Y_train.values.ravel())\n",
        "\n",
        "#print the optimisation results\n",
        "def display(results):\n",
        "    print(f'Best parameters are: {results.best_params_}')\n",
        "    print(\"\\n\")\n",
        "    mean_score = results.cv_results_['mean_test_score']\n",
        "    std_score = results.cv_results_['std_test_score']\n",
        "    params = results.cv_results_['params']\n",
        "    for mean,std,params in zip(mean_score,std_score,params):\n",
        "        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')\n",
        "\n",
        "display(cv)\n",
        "\n",
        "#train the model with the optimised parameters\n",
        "model = RandomForestRegressor(max_depth = 4, n_estimators = 50, random_state =0)\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "#create the file in the right format for upload\n",
        "Y_pred = pd.DataFrame(data=Y_pred)\n",
        "Y_pred.columns = ['pop']\n",
        "Id = regtestdf.loc[:, \"Id\" : \"Id\"] \n",
        "result = pd.concat([Id, Y_pred,], axis=1, sort=False)\n",
        "result = result.set_index('Id')\n",
        "result.to_csv('randomforest_normalize.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qxEfhrvFlNQ",
        "colab_type": "text"
      },
      "source": [
        "#### *2.4 Support Vector Regression*\n",
        "\n",
        "The Support Vector Regression (SVR) is built on a similar approach as the Support Vector Machine (SVM) Classification model. The goal of the SVM is to find the hyperplane that maximises the margin between the observations and avoiding margin violations (hard margin) or limiting them (soft margin). The SVR tries to fit the error rates within the margin and maximise the margin.\n",
        "The grid search method was used to find the optimal hyperparameters for the model. The optimal parameters for the SVR model were a polynomial kernel (kernel='poly'), a kernel coefficient of 0.6 (gamma = 0.6) and a strength of regularisation of 1000 (C = 1000). Again, the normalisation data scaling technique was applied. The final Kaggle Public RMSE was 7.36986 in comparison to a RMSE of 7.84204 using the Robust Scaler method.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze4TQdGvFrQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#4.4 Support Vector Regression\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing\n",
        "\n",
        "#importing the train and the test data\n",
        "#drop the nan value\n",
        "regtraindf = training_data_reg\n",
        "regtraindf.dropna()\n",
        "regtestdf = test_data_reg\n",
        "regtestdf.dropna()\n",
        "\n",
        "#split the sets in train set and test set and remove variables not used for the regression problems\n",
        "X_train = regtraindf.loc[:, \"nrgy\" : \"acous\"]\n",
        "X_train = X_train.drop(['live','val','nrgy','dnce'], axis=1)\n",
        "Y_train = regtraindf.loc[:, \"pop\":\"pop\"]\n",
        "X_test = regtestdf.loc[:, \"nrgy\" : \"acous\"]\n",
        "X_test = X_test.drop(['live','val','nrgy','dnce'], axis=1)\n",
        "\n",
        "#Different data scaling techniques\n",
        "#results in RMSE of 7.36986\n",
        "X_test = preprocessing.normalize(X_test)\n",
        "X_train = preprocessing.normalize(X_train)\n",
        "\n",
        "# =============================================================================\n",
        "# #Scale the data\n",
        "# #results in RMSE of 7.93718\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_train = pd.DataFrame(data=X_train)\n",
        "# X_test = scaler.fit_transform(X_test)\n",
        "# X_test = pd.DataFrame(data=X_test)\n",
        "# \n",
        "# #results in RMSE of 7.66126\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "# scaler = RobustScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_train = pd.DataFrame(data=X_train)\n",
        "# X_test = scaler.fit_transform(X_test)\n",
        "# X_test = pd.DataFrame(data=X_test)\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "#find the optimal parameters\n",
        "SVR_M = SVR()\n",
        "#define parameters for optimisation\n",
        "parameters = {\n",
        "    \"gamma\" : [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5, 0.6, 0.9],\n",
        "    \"kernel\" : ['rbf', 'poly', 'linear', 'sigmoid'],\n",
        "    \"C\": [1, 10, 100, 1000, 10000]\n",
        "    \n",
        "}\n",
        "\n",
        "#import the packages for the GridSearchCV optimisaion and import mean_squared_error scorer\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "cv = GridSearchCV(SVR_M, parameters, scoring=scorer)\n",
        "cv.fit(X_train,Y_train.values.ravel())\n",
        "\n",
        "#print the optimisation results\n",
        "def display(results):\n",
        "    print(f'Best parameters are: {results.best_params_}')\n",
        "    print(\"\\n\")\n",
        "    mean_score = results.cv_results_['mean_test_score']\n",
        "    std_score = results.cv_results_['std_test_score']\n",
        "    params = results.cv_results_['params']\n",
        "    for mean,std,params in zip(mean_score,std_score,params):\n",
        "        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')\n",
        "\n",
        "display(cv)\n",
        "\n",
        "#train the model with the optimised parameters\n",
        "SVR_model = SVR(kernel='poly', C = 1000, gamma = 0.6)\n",
        "SVR_model.fit(X_train, Y_train)\n",
        "Y_pred = SVR_model.predict(X_test)\n",
        "\n",
        "#create the file in the right format for upload\n",
        "Y_pred = pd.DataFrame(data=Y_pred)\n",
        "Y_pred.columns = ['pop']\n",
        "Id = regtestdf.loc[:, \"Id\" : \"Id\"] \n",
        "result = pd.concat([Id, Y_pred,], axis=1, sort=False)\n",
        "result = result.set_index('Id')\n",
        "result.to_csv('SVR_normalize.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxle4_ddFrxZ",
        "colab_type": "text"
      },
      "source": [
        "### **3. Conclusion & Reflection**\n",
        "\n",
        "What this analysis clearly shows is that the results of the regression models can only be as good as the input data. Before starting this report, we did not expect that scaling of the data can improve the results so significantly. What this report demonstrates is that extensive pre-processing of the input data can go a long way.\n",
        "\n",
        "Deciding on the right model is definitely important but also fine tuning the model is a crucial step. By tuning the SVR and Random Forest model the final results fall in the same area of 7.36986 and 7.23191 respectively. It is important to highlight that the focus of the regression task clearly laid on minimising the RMSE. However, it is also essential to consider other parameters like the MAE, R-Squared or F-Test to evaluate the models in greater detail. Additionally, employing other techniques as deep learning via neural networks might further increase the precision in predicting the popularity of Spotify songs.\n",
        "The regression task is an up-to-date problem that many companies and institutions face at the moment. As Quantitative Finance students we will definitely be able to transfer our experience to other point estimation exercises as predicting stock prices or forecasting company revenues.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pexEYyOFyej",
        "colab_type": "text"
      },
      "source": [
        "## II. Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDBA6i3aF2f8",
        "colab_type": "text"
      },
      "source": [
        "### **1. Introduction**\n",
        "\n",
        "Spotify is the second biggest provider of music streaming services in the world in terms of monthly user base shadowed only by Apple Music. With their vast collection of tracks spanning across genres on the edge of the creative frontier, predicting musical taste and providing salient recommendations to the people who use Spotify’s services has never been more challenging.\n",
        "A good music recommender system focuses on recognising the genre and the corresponding mood of the song so that it can make situation-specific suggestions for the next one. As a purely abstract medium, music proves very hard to classify in terms of genre because of its many nuances. These nuances ultimately contribute to its high dimensionality in terms of attributes that can be explored as classifiers. What is more, suggesting the most representative song from a corpus of titles within a genre is what sets the scene of the listening session thus allowing for its gradual exploration. In order to do that the recommender system is interested in quantifying the absolute popularity of a song. Apart from the main features of the system, recently Spotify has introduced the service Tastebreakers which allows for the exploration of the complement to the taste of the listener thus pushing them outside of their comfort zone – another feature relying heavy on classification and quantifying.\n",
        "Based on these concepts, the current work is interested in exploring different machine learning approaches to predicting a song’s genre and popularity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F-P3XJdGAFp",
        "colab_type": "text"
      },
      "source": [
        "#### *1.1 Description of the data set*\n",
        "\n",
        "For the purposes of the analysis the following libraries were used: pandas, numpy, seaborn, statsmodels, and sklearn. The environment sellected for the development and testing of the script was Colab.\n",
        "\n",
        "The data for the analysis was taken from the Spotify Songs by Decade list and consists of 4 sets divided into data for regression and classification each subdivided into sets for training and testing. Both training sets include 453 songs with 15 attributes and the test sets consisted of 114 (regression set) and 113 (classification set) with 14 attributes.\n",
        "The attributes provided included:\n",
        "\n",
        "* Id - an arbitrary unique track identifier \n",
        "* title - track title \n",
        "* artist - singer or band \n",
        "* top genre - genre of the track \n",
        "* year - year of release (or re-release) \n",
        "* bpm - beats per minute (tempo) \n",
        "* nrgy - energy: the higher the value the more energetic \n",
        "* dnce - danceability: the higher the value, the easier it is to dance to this song \n",
        "* dB - loudness (dB): the higher the value, the louder the song \n",
        "* live - liveness: the higher the value, the more likely the song is a live recording \n",
        "* val - valence: the higher the value, the more positive mood for the song \n",
        "* dur - duration: the length of the song \n",
        "* acous - acousticness: the higher the value the more acoustic the song is \n",
        "* spch - speechiness: the higher the value the more spoken word the song contains \n",
        "* pop - popularity: the higher the value the more popular the song is (and the target variable for this problem)\n",
        "\n",
        "During the exploration of the data it was noted that some of the attributes were unevenly distributed, with live and speechiness resembling a logarithmic distribution, and year having strong positive skew (Fig. 1). These observations were explained by the facts that all songs have a minimum of 0:01s playtime, some songs are instrumental, and that modern technology allows for more music to be made more affordably thus causing a greater creative output which ultimately pushes the numbers up as data moves towards the present day.\n",
        "\n",
        "To identify relationships between the attributes for the classification task, a correlation matrix was created and the correlations between popularity and the rest of the predictors were used as a guide when constructing the models. In the course of the analysis only the strongest classifiers were used (Fig. 2).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCpaVpMeF7k6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the Data\n",
        "# from google.colab import files # upload json token\n",
        "# files.upload()\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/kaggle.json\n",
        "# !chmod 600 /root/.kaggle/kaggle.json\n",
        "# !pip install kaggle\n",
        "# !kaggle competitions download -c cs98x-spotify-regression\n",
        "# !kaggle competitions download -c cs98xspotifyclassification\n",
        "\n",
        "# Libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import sklearn as skl\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import ensemble\n",
        "from sklearn import linear_model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# This is what we train the regression model on\n",
        "training_data_reg = pd.read_csv('/content/CS98XRegressionTrain.csv', low_memory = False)\n",
        "test_data_reg = pd.read_csv('/content/CS98XRegressionTest.csv', low_memory = False)\n",
        "# This is what we train the classification model on\n",
        "training_data_class = pd.read_csv('/content/CS98XClassificationTrain.csv', low_memory = False)\n",
        "test_data_class = pd.read_csv('/content/CS98XClassificationTest.csv', low_memory = False)\n",
        "\n",
        "# Histograms\n",
        "for_hist = training_data_reg.select_dtypes(exclude=['object'])\n",
        "for_hist.drop(columns='Id', inplace=True)\n",
        "for_hist.hist(column= for_hist.columns, figsize=(20,14))\n",
        "plt.savefig('hist.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4tBP41xTKbd",
        "colab_type": "text"
      },
      "source": [
        "Figure 1: Distributions of the attributes\n",
        "![Histograms](https://raw.githubusercontent.com/TheodorIvanov/ML_Group_J/master/hist.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJJI6UyiTtYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing NaNs\n",
        "training_data_reg = training_data_reg.dropna()\n",
        "norm_training_data_reg = pd.DataFrame(preprocessing.normalize(training_data_reg.select_dtypes(['number']))) # Normalising\n",
        "# Correlation heatmap\n",
        "training_data_reg_cov = training_data_reg.drop(columns=['Id','year']).corr() # # Removing non-predictor columns\n",
        "plt.figure(figsize =(14,11))\n",
        "plt.title('Predictor Correlations')\n",
        "sns.heatmap(training_data_reg_cov, annot=True, center=0, cmap='BrBG_r')\n",
        "plt.savefig('corr.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INqMcrTbTt-T",
        "colab_type": "text"
      },
      "source": [
        "Figure 2: Correlation matrix between the attributes\n",
        "![Correlation Matrix](https://raw.githubusercontent.com/TheodorIvanov/ML_Group_J/master/corr.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf7bhFzqNFuy",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxiVWGGUI7Rt",
        "colab_type": "text"
      },
      "source": [
        "### **2. Classification Models and Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVDyIm3YI_to",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1 Support Vector Machine\n",
        "A very prominent supervised machine learning algorithm that performs classification is the Support Vector Classifier (SVC). It is used to find a decision boundary in more than two dimensions that divides the given data into different categories (here: top genres) and is suitable for small datasets. \n",
        "The decision boundary is found through the maximum margin, i.e. the distance between data points of different classes. To find the most accurate prediction score, the three hyper-parameters of the SVC (C, kernel and gamma) were optimized by importing the GridSearchCV from sklearn.modelapproach.\n",
        "Grid Search evaluates several combinations of algorithm parameters.\n",
        "To transform the input from a low-dimensional space into a higher-dimensional space, the Grid Search determines a sigmoid kernel approach to separate the data with a non-linear hyperplane.\n",
        "In addition, Grid Search proposes a low 0.01 gamma value, meaning that far away data points also be considered to get the decision boundary. A low gamma can also help to prevent overfitting.\n",
        "Lastly, the parameter tuning recommends a regularization parameter C of 10, which again is quite low and tries to stop overfitting.\n",
        "The Classifier has been trained with 11 scaled numerical features only (the columns 'title', 'artist' were removed from the training and test dataset) and achieved a performance Kaggle Public score of 0.33928.\n",
        "Further adjustments such as removing different columns (i.e. year or val), increasing the value of gamma and encoding the column ‘artist’ to use it for prediction did not provide a higher score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5_9SQIeI_bU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set-Up\n",
        "#import libraries and datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn import preprocessing\n",
        "from sklearn.svm import SVC \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#training_data_class\n",
        "train = pd.read_csv('/content/CS98XClassificationTrain.csv', low_memory = False) \n",
        "#test_data_class\n",
        "test = pd.read_csv('/content/CS98XClassificationTest.csv', low_memory = False)\n",
        "train.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
        "\n",
        "#rename the column to avoid error\n",
        "train = train.rename(columns = {'top genre' : 'topgenre'})\n",
        "\n",
        "##remove some columns that are not used in this project\n",
        "train = train.drop('artist', axis = 1)\n",
        "train = train.drop('title', axis = 1)\n",
        "test = test.drop('artist', axis = 1)\n",
        "test = test.drop('title', axis = 1)\n",
        "\n",
        "#scale train and test data\n",
        "train[['topgenre']] = train[['topgenre']].astype(str)\n",
        "trainfeat = train.drop('topgenre', axis =1)\n",
        "scaledfeat = preprocessing.scale(trainfeat)\n",
        "trainoutcome = train.topgenre #what we predict\n",
        "scaledtest = preprocessing.scale(test)\n",
        "\n",
        "#find optimal parameters for svc\n",
        "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
        "\n",
        "#apply param_grid\n",
        "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
        "grid.fit(scaledfeat, trainoutcome)\n",
        "\n",
        "print(grid.best_estimator_)\n",
        "\n",
        "svc = SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
        "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='sigmoid',\n",
        "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "    tol=0.001, verbose=False)\n",
        "\n",
        "svc.fit(scaledfeat, trainoutcome)\n",
        "cross_val_score(svc, scaledfeat, trainoutcome, cv = 5, scoring = 'accuracy' )\n",
        "\n",
        "#predict genre\n",
        "clean_test_class = test.dropna()\n",
        "y_pred = svc.predict(scaledtest)\n",
        "yresult = pd.DataFrame()\n",
        "yresult['top genre'] = y_pred.T\n",
        "predict = pd.merge(yresult['top genre'], clean_test_class, left_index=True, right_index=True)\n",
        "predict = predict[['Id',  'top genre']]\n",
        "predict.to_csv('svc.csv')\n",
        "\n",
        "#!kaggle competitions submit -c cs98x-spotify-regression -f svc.csv -m \"SVC\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRavgYMdI2cr",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2 Decision Trees\n",
        " \n",
        "Decision Trees are a popular approach in supervised machine learning when it comes to classification since they are far less sensitive to noise and irrelevant/missing values than other classifiers.\n",
        "The Decision Tree Classifier (DTC) tries to find a way to split the training data until all the data has been classified into certain classes (here: top genres).\n",
        "To measure the quality of those splits, the DTC's parameter 'criterion' can either be Gini (for Gini impurity) or entropy (for information gain). To decide which criterion fits best to predict the top genre of a song, the Grid Search was used. It recommends applying the entropy criteria which may compute a little slower than the Gini index since entropy is computing a logarithmic function.\n",
        "Secondly, Grid Search proposes a maximum depth of 4 meaning that the length of the paths from the root of the tree to the leaf is no greater than 4.\n",
        "To see whether the parameter tuning increases the accuracy score, the DTC was applied with and without adjusted parameters. \n",
        "Given the same data and attributes which were already used with the SVC, the DTC scores an accuracy between 28% and 29% at most.\n",
        "After applying a maximum depth of 4 and an entropy criterion, the accuracy indeed increases. Given the optimized parameters, the DTC escalates to 32%, which still less than the SVC score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrSee6HfJOBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import libraries and Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import decomposition, datasets\n",
        "from sklearn import tree\n",
        "\n",
        "#define variables\n",
        "X = scaledfeat\n",
        "y = trainoutcome\n",
        "tree_clf = DecisionTreeClassifier(max_depth=5)\n",
        "tree_clf.fit(X,y)\n",
        "\n",
        "sc = StandardScaler()\n",
        "pca = decomposition.PCA()\n",
        "decisiontree = tree.DecisionTreeClassifier()\n",
        "pipe = Pipeline(steps=[('sc', sc),\n",
        "                       ('decisiontree', decisiontree)])\n",
        "\n",
        "#define parameters which we want to tune\n",
        "criterion = ['gini', 'entropy']\n",
        "max_depth = [4,6,8,12]\n",
        "parameters = dict(decisiontree__criterion=criterion,\n",
        "                  decisiontree__max_depth=max_depth)\n",
        "\n",
        "tree_clf = GridSearchCV(pipe, parameters)\n",
        "tree_clf.fit(X, y)\n",
        "\n",
        "#print best parameter values\n",
        "print('Best Criterion:', tree_clf.best_estimator_.get_params()['decisiontree__criterion'])\n",
        "print('Best max_depth:', tree_clf.best_estimator_.get_params()['decisiontree__max_depth'])\n",
        "print(); print(tree_clf.best_estimator_.get_params()['decisiontree'])\n",
        "\n",
        "#adapt best parameters\n",
        "tree.clf = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                       max_depth=4, max_features=None, max_leaf_nodes=None,\n",
        "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                       min_samples_leaf=1, min_samples_split=2,\n",
        "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                       random_state=None, splitter='best')\n",
        "\n",
        "CV_Result = cross_val_score(tree.clf, X, y, cv=3, n_jobs=-1)\n",
        "print(); print(CV_Result)\n",
        "print(); print(CV_Result.mean())\n",
        "print(); print(CV_Result.std())\n",
        "\n",
        "#show accuracy of DecisionTreeClassifier\n",
        "cross_val_score(svc, scaledfeat, trainoutcome, cv = 5, scoring = 'accuracy' )\n",
        "\n",
        "#Predict top genre using the test dataset\n",
        "y_pred = svc.predict(scaledtest)\n",
        "yresult = pd.DataFrame()\n",
        "yresult['top genre'] = y_pred.T\n",
        "predict = pd.merge(yresult['top genre'], clean_test_class, left_index=True, right_index=True)\n",
        "predict = predict[['Id',  'top genre']]\n",
        "predict.to_csv('decision.csv')\n",
        "\n",
        "#!kaggle competitions submit -c cs98xspotifyclassification -f decision.csv -m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGpDEMj3JOcy",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3 Gradient Boosting: Classification\n",
        "\n",
        "Gradient Boosting is a hypothesis boosting technique which aims at sequentially introducing predictors to an ensemble of weak learners. Unlike its close relative the Adaptive Boosting which focuses on altering the weight of the misclassified instances during training, the Gradient Boost alters the weight of the residual errors of the previous predictor. It uses a number of decision trees as base learners and can be applied to both classification and regression problems.\n",
        "\n",
        "In terms of predicting the genre of the songs in our case, the Gradient Boosting Classifier class was used by training it with the most of the numeric data set attributes (excluding year, id, valence, and liveliness) as well as hot one encoded version of the artist name since as established earlier in the preliminary analysis, artists have high predictive power due to their genre loyalty. After performing iterative optimisation over the parameters of the model, the final criteria converged on 1000 estimators at a learning rate of 0.1, with a total of 8 features and a tree depth of 2. The performance score for this model was 100% for the training set and 47.3% for the validation. The superior performance of this model to the others deployed throughout the research process yielded a Kaggle Public score of 50% and is the one chosen to represent this work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFj6GD9QJez1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Gradient Boosting: Classification\n",
        "train =  pd.read_csv('/content/CS98XClassificationTrain.csv', low_memory = False)\n",
        "test = pd.read_csv('/content/CS98XClassificationTest.csv', low_memory = False)\n",
        "train.set_index(\"Id\", inplace=True)\n",
        "test.set_index(\"Id\", inplace=True)\n",
        "train.dropna(inplace=True)\n",
        "test.dropna(inplace=True)\n",
        "y_train = train[\"top genre\"] # assigning training results\n",
        "\n",
        "train.drop(labels=\"top genre\", axis=1, inplace=True)\n",
        "train_test =  train.append(test)\n",
        "\n",
        "# delete columns that are not used as features for training and prediction\n",
        "columns_to_drop = ['title', 'year', 'val', 'live']\n",
        "train_test.drop(labels=columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "# converting objects to numbers (hot-one encoding)\n",
        "train_test_dummies = pd.get_dummies(train_test, columns=[\"artist\"])\n",
        "\n",
        "# replace nulls with 0.0\n",
        "train_test_dummies.fillna(value=0.0, inplace=True)\n",
        "\n",
        "# generate feature sets (X)\n",
        "X_train = train_test_dummies.values[0:438]\n",
        "X_test = train_test_dummies.values[438:]\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scale = scaler.fit_transform(X_train)\n",
        "X_test_scale = scaler.transform(X_test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# splitting the data\n",
        "X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train_scale, y_train, random_state=0, shuffle=None)\n",
        "\n",
        "# import machine learning algorithms\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# train with Gradient Boosting algorithm compute the accuracy scores on train and validation sets when training with different learning rates\n",
        "learning_rates = [0.05, 0.1]\n",
        "for learning_rate in learning_rates:\n",
        "    gb = GradientBoostingClassifier(n_estimators=200, learning_rate = learning_rate, max_features=8, max_depth = 2, random_state = 0)\n",
        "    gb.fit(X_train_sub, y_train_sub)\n",
        "    print(\"Learning rate: \", learning_rate)\n",
        "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
        "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
        "    print()\n",
        "\n",
        "# outputing confusion matrix and classification report of Gradient Boosting algorithm on validation set\n",
        "gb = GradientBoostingClassifier(n_estimators=1000, learning_rate = 0.1, max_features=8, max_depth = 2, random_state = 0) # set estimator to 1000\n",
        "gb.fit(X_train_sub, y_train_sub)\n",
        "predictions = gb.predict(X_test_scale)\n",
        "print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
        "print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
        "\n",
        "# Submitting results to Kaggle\n",
        "class_result = pd.DataFrame()\n",
        "class_result['top genre'] = predictions.T\n",
        "class_result['Id'] = test.index\n",
        "class_result.index = class_result['Id']\n",
        "class_result.drop(columns='Id',inplace=True)\n",
        "class_result.to_csv('class_enc_boost.csv')\n",
        "\n",
        "#!kaggle competitions submit -c cs98xspotifyclassification -f class_enc_boost.csv -m \"Encoded boosting\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Pu0o0qNAsb",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ba-Qw8kJnmN",
        "colab_type": "text"
      },
      "source": [
        "### **3. Conclusion & Reflection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5xJVh7IbEB7",
        "colab_type": "text"
      },
      "source": [
        "The wide variety of models used during this analysis yielded various results. It must be noted that despite its promising performance, Gradient Boosting comes at a high computational cost. This is due to the fact that in its core Gradient Boosting is a linear algorithm which requires sequential processing and is thus hard to spread over a multithreaded architecture. Thinking from a scalability perspective, this adds an extra layer of complexity when deploying in a real-life scenario, and in the context of song classification it can prove difficult to implement in a dynamic environment with new songs being released daily. What is more, as in the regression example, the quality and scaling of the data is of paramount importance to the performance of the model.\n",
        "\n",
        "Looking forward, a replication of this work will greatly benefit from exploring the application of neural network models to the classification of song genres. Being far more dynamic in terms of accommodating new instances and taking the full advantage of cutting-edge multithreaded hardware, neural networks are a solution that an ever-growing number of companies focus their resources on, one which can be even better suited to the current task.\n",
        "\n",
        "---"
      ]
    }
  ]
}